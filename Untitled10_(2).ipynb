{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shilu10/Machine_Learning/blob/main/Untitled10_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuEUoxBAiE4u"
      },
      "outputs": [],
      "source": [
        "# imports \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Text Preprocessing\n",
        "import nltk\n",
        "# nltk.download(\"all\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# for the pictures\n",
        "from wordcloud import WordCloud\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2llpX7G3iTfE"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/SMSSpamCollection.txt', header = None, delimiter = '\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuaEQkd3QYdB"
      },
      "source": [
        "**Exploratory Data Analysis (EDA)**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttoELstYiz3H"
      },
      "outputs": [],
      "source": [
        "df.columns = ['Label', 'Messages']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgATW7-YjEg8"
      },
      "outputs": [],
      "source": [
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOxU0tDRjPpf"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifH85G3DkDf5"
      },
      "source": [
        "There is no missing values in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-coZsbpjQHI"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pAgFUDhqzby"
      },
      "source": [
        "Both columns are Categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVOZlWZLjQSQ"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGl3e4pFrnY9"
      },
      "source": [
        "From freq row, it seems there are dupliated records in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQkV-gA9jQrv"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNIoo62DjQWs"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()]['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ApBwcvs1ZmK"
      },
      "outputs": [],
      "source": [
        "df[\"Label\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n",
        "plt.xlabel(\"Spam vs Ham\")\n",
        "plt.legend([\"Ham\", \"Spam\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjoS3ibHs35W"
      },
      "source": [
        "We can able to see that, there are 403 columns has a duplication, And out of 403 records, 309 are ham message records and 94 spam message records. This says that some messages are been repeated by the peoples often.In this type of problems duplicated messages are play a vital role for helping the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvh-rHt9t7f0"
      },
      "source": [
        "It is an Imbalanced dataset, we can see the ratio of the class as 8:2 .So if we train our model with this dataset. There is an high probability that our model will be biased towards Ham message class, Because most of the records are from Ham Message class. We will first take care of the duplicates, and we can try to handle imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfLrJ7RhtSed"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x = df['Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PBguSQ3tSjb"
      },
      "outputs": [],
      "source": [
        "# looking for the most repeated words in both spam and ham message class.\n",
        "df[df.duplicated()].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLlJRgehnDRk"
      },
      "outputs": [],
      "source": [
        "repeated_messages = df.groupby(\"Messages\")['Label'].value_counts().nlargest(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HbEhMIdnDWs"
      },
      "outputs": [],
      "source": [
        "#The most repeated messages .\n",
        "repeated_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eA3HYvUQnDbj"
      },
      "outputs": [],
      "source": [
        "spam_messages = df[ df['Label'] == 'spam' ]\n",
        "ham_messages = df[ df['Label'] == 'ham' ]\n",
        "spam_words = [ ]\n",
        "ham_words = [ ]\n",
        "\n",
        "def spamword_extraction(message) :\n",
        "  global spam_words\n",
        "  words = [word.lower() for word in word_tokenize(message) if word.lower() not in stopwords.words('english') and word.lower().isalpha()]\n",
        "  spam_words += words\n",
        "\n",
        "def hamword_extraction(message) :\n",
        "  global ham_words\n",
        "  words = [word.lower() for word in word_tokenize(message) if word.lower() not in stopwords.words('english') and word.lower().isalpha()]\n",
        "  ham_words += words\n",
        "\n",
        "spam_messages.Messages.apply(spamword_extraction)\n",
        "ham_messages.Messages.apply(hamword_extraction)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwTN4ock6toI"
      },
      "source": [
        "We are just trying to collect the spam words and ham words separately, so we applied word_tokenize to get the words from the sentences and stopwords to remove the stop words like repeated words which makes nosense for our project like it,the,i,etc..isaplha() method is useful to extract the words only has a alphabets in english."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-NAqkhinsIe"
      },
      "outputs": [],
      "source": [
        "# Ham Words cloud.\n",
        "ham_word_cloud = WordCloud(width=800, height=500).generate(\" \".join(ham_words))\n",
        "plt.figure(figsize = (12,10))\n",
        "plt.imshow(ham_word_cloud)\n",
        "plt.tight_layout(pad = 0)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AQVCTGansOQ"
      },
      "outputs": [],
      "source": [
        "# Spam word cloud\n",
        "spam_word_cloud = WordCloud(width=800, height=500).generate(\" \".join(spam_words))\n",
        "plt.figure(figsize = (12,10))\n",
        "plt.imshow(spam_word_cloud)\n",
        "plt.tight_layout(pad = 0)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8E0kzV3nsV3"
      },
      "outputs": [],
      "source": [
        "# Iam going to create a new dataset with handling the imbalanced data, i will try to compare the results between the imbalanced and \n",
        "# balanced dataset.\n",
        "print(f\"Number of Spam Message records {len(spam_messages)}\")\n",
        "print(f\"Number of Ham Message records {len(ham_messages)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jns215CkCeNY"
      },
      "outputs": [],
      "source": [
        "# repeated words in both of the classes\n",
        "# repeated words in the spam class \n",
        "from collections import Counter\n",
        "repeated_spamwords = Counter(spam_words).most_common(25)\n",
        "repeated_spamwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UhUlWJvCeXY"
      },
      "outputs": [],
      "source": [
        "# repeated words in ham class\n",
        "repeated_hamwords = Counter(ham_words).most_common(25)\n",
        "repeated_hamwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PpZvFV8Ceej"
      },
      "outputs": [],
      "source": [
        "# message length before data preprocessing.\n",
        "df['Messagelength_before'] = df['Messages'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfmXxAeQJqWx"
      },
      "outputs": [],
      "source": [
        "# messages length before text preprocessing.\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPQuN49LJqdN"
      },
      "outputs": [],
      "source": [
        "spam_message_len = df[df['Label'] == 'spam'].Messagelength_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy8N389HJqj4"
      },
      "outputs": [],
      "source": [
        "spam_message_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj9jzpfsJqpz"
      },
      "outputs": [],
      "source": [
        "ham_message_len = df[df['Label'] == 'ham'].Messagelength_before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTYG5rvzJqyE"
      },
      "outputs": [],
      "source": [
        "ham_message_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUJKOSm6OCfe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 8))\n",
        "sns.histplot(spam_message_len, kde = True, bins = 100)\n",
        "plt.xlabel('MessageLength')\n",
        "plt.title('Spam Message Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0lT3TiLPY9O"
      },
      "source": [
        " Spam messages length are right skewed, which shows that the message, most of the messages length are higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n8bHagiOClk"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10, 8))\n",
        "sns.histplot(ham_message_len, kde = True, bins = 100)\n",
        "plt.xlabel('MessageLength')\n",
        "plt.title('Ham Message Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4-KPt72OCr6"
      },
      "outputs": [],
      "source": [
        "spam_message_len.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-71JOavCOCw7"
      },
      "outputs": [],
      "source": [
        "ham_message_len.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVOnGCRXQTBa"
      },
      "source": [
        "From both Spam and Ham messages, we can notice that 25 % of messages length in ham messages are 33 and 75 % are 93 length which is lesser than the 25 % of the spam message length, So we can observe that the spam message length is most of the time higher than the ham message length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDGy9L6jQ0-v"
      },
      "source": [
        "**Feature Engineering**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8lqK2zaOC2g"
      },
      "outputs": [],
      "source": [
        "# text preprocessing\n",
        "def text_preprocessing(message) :\n",
        "  # removing a numbers from the message\n",
        "  pattern = r'[0-9]'\n",
        "  message = re.sub(pattern, ' ', message)\n",
        "\n",
        "  # removing special character and punctuations and every other things from the message.\n",
        "  message = re.sub('[^A-Za-z0-9]+', ' ', message)\n",
        "\n",
        "  #applying a stop word \n",
        "  message = [word for word in word_tokenize(message)]\n",
        "  words = [word.lower() for word in message if not word in stopwords.words('english')]\n",
        "  message = ' '.join(words)\n",
        "\n",
        "  return message\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us9r6qZQOC8i"
      },
      "outputs": [],
      "source": [
        "df['NewMessage'] = df['Messages'].apply(text_preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U3x6n7SODBu"
      },
      "outputs": [],
      "source": [
        "df['NewMessageLen'] = df['NewMessage'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q_aUwF9ODHX"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ypHy40XVpEb"
      },
      "source": [
        "After doing a text preprocessing length of the messages of both spam and ham classes are decreased, So now also we will check whether the distribution of the messages length of both classes still same like before doing the text preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vp3O0SrnODM4"
      },
      "outputs": [],
      "source": [
        "spam_message_len_after = df[df['Label'] == 'spam'].NewMessageLen\n",
        "plt.figure(figsize = (10, 8))\n",
        "sns.histplot(spam_message_len_after, kde = True, bins = 100)\n",
        "plt.xlabel('MessageLength')\n",
        "plt.title('Spam Message Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02Wn2N0hODS8"
      },
      "outputs": [],
      "source": [
        "ham_message_len_after = df[df['Label'] == 'ham'].NewMessageLen\n",
        "plt.figure(figsize = (10, 8))\n",
        "sns.histplot(ham_message_len_after, kde = True, bins = 100)\n",
        "plt.xlabel('MessageLength')\n",
        "plt.title('ham Message Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uEzF5LYODYg"
      },
      "outputs": [],
      "source": [
        "spam_message_len_after.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5smIt5WODdz"
      },
      "outputs": [],
      "source": [
        "ham_message_len_after.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dedHJxGXIGZ"
      },
      "source": [
        "So still after the text preprocessing ham messages length 75 % is still lesser than the 25 % of the spam messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "draHJn7kODjY"
      },
      "outputs": [],
      "source": [
        "# converting the text into words and words into the vectors, bcoz machine learning and deep learning model only understand the numerical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M5JAAWXODow"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcfjrEinnDzE"
      },
      "outputs": [],
      "source": [
        "df.drop('Messages', axis = 1, inplace = True)\n",
        "df['Messages'] = df['NewMessage']\n",
        "df.drop('NewMessage', inplace = True, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gElohEhDnD3V"
      },
      "outputs": [],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymaJOINucJLv"
      },
      "outputs": [],
      "source": [
        "y = df['Label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP5tQV6Tdg0C"
      },
      "outputs": [],
      "source": [
        "y = np.where(y == 'ham', 0, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xC1vs3CRdg7U"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0ELlMRmdhCY"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "countvectorizer = CountVectorizer(max_features = 5000)\n",
        "X = countvectorizer.fit_transform(df['Messages']) .toarray()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJqZGp6WdhI5"
      },
      "outputs": [],
      "source": [
        "# these are the features used in the bag of words\n",
        "#countvectorizer.get_feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUJstA2cyY15"
      },
      "source": [
        "#**Model selection with Imbalanced Dataset** \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw9vcFwAZ4zT"
      },
      "source": [
        "# Performance of the models is calculated using the F1-Score.\n",
        "1. Multinomial Naive Bayes ---> 0.938\n",
        "2. Random Forest Classifier ---> 0.909\n",
        "3. Support Vector Machine Classifier ---> 0.914"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on2obvQ8dhPD"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FgjSEXTxizO"
      },
      "outputs": [],
      "source": [
        "# MultiNomial Naive Bayes\n",
        "multinb = MultinomialNB()\n",
        "multinb.fit(x_train, y_train)\n",
        "y_pred = multinb.predict(x_test)\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijEk6sWVcqvj"
      },
      "outputs": [],
      "source": [
        "print(\"-----Confusion Matrix for MultiNomial Naive Bayes with imbalanced dataset----\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8,5))\n",
        "axis_labels = ['ham', 'spam']\n",
        "ga = sns.heatmap(data=cm, annot=True, cmap=\"Blues\", xticklabels=axis_labels, yticklabels=axis_labels, fmt='g', cbar_kws={\"shrink\": 0.5})\n",
        "plot = plt.xlabel('Actual values')\n",
        "plot = plt.ylabel('Predicted values')\n",
        "plot = plt.title('MultiNomial Naive Bayes')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(\"-----classification report-------\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print()\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_scores = cross_val_score(multinb, X, y, cv = 10, scoring = 'f1')\n",
        "print(f\"Scores for Multinomial Model with imbalanced data is {cross_val_scores}\")\n",
        "print()\n",
        "\n",
        "print(f\"Average score for the Multinomial with imbalanced data is {cross_val_scores.mean()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyvqQ0WQjVdK"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "randomforest = RandomForestClassifier()\n",
        "randomforest.fit(x_train, y_train)\n",
        "y_pred_randomforest = randomforest.predict(x_test)\n",
        "\n",
        "# confusion matrix\n",
        "print(\"-----Confusion Matrix for Random Forest with imbalanced dataset----\")\n",
        "cm = confusion_matrix(y_test, y_pred_randomforest)\n",
        "plt.figure(figsize=(8,5))\n",
        "axis_labels = ['ham', 'spam']\n",
        "ga = sns.heatmap(data=cm, annot=True, cmap=\"Blues\", xticklabels=axis_labels, yticklabels=axis_labels, fmt='g', cbar_kws={\"shrink\": 0.5})\n",
        "plot = plt.xlabel('Actual values')\n",
        "plot = plt.ylabel('Predicted values')\n",
        "plot = plt.title('Random Forest')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(\"-----classification report-------\")\n",
        "print(classification_report(y_test, y_pred_randomforest))\n",
        "print()\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_scores = cross_val_score(randomforest, X, y, cv = 10, scoring = 'f1')\n",
        "print(f\"Scores for Random Forest with imbalanced data is {cross_val_scores}\")\n",
        "print()\n",
        "\n",
        "print(f\"Average score for the Random Forest with imbalanced data is {cross_val_scores.mean()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56tliUbbjVXZ"
      },
      "outputs": [],
      "source": [
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "supportv_classifier = SVC()\n",
        "supportv_classifier.fit(x_train, y_train)\n",
        "y_pred_svm = supportv_classifier.predict(x_test)\n",
        "\n",
        "\n",
        "# confusion matrix\n",
        "print(\"-----Confusion Matrix for Support Vector Machine with imbalanced dataset----\")\n",
        "cm = confusion_matrix(y_test, y_pred_svm)\n",
        "plt.figure(figsize=(8,5))\n",
        "axis_labels = ['ham', 'spam']\n",
        "ga = sns.heatmap(data=cm, annot=True, cmap=\"Blues\", xticklabels=axis_labels, yticklabels=axis_labels, fmt='g', cbar_kws={\"shrink\": 0.5})\n",
        "plot = plt.xlabel('Actual values')\n",
        "plot = plt.ylabel('Predicted values')\n",
        "plot = plt.title('Support Vector Machine')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(\"-----classification report-------\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "print()\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_scores = cross_val_score(supportv_classifier, X, y, cv = 10, scoring = 'f1')\n",
        "print(f\"Scores for Svm Model with imbalanced data is {cross_val_scores}\")\n",
        "print()\n",
        "\n",
        "print(f\"Average score for the Svm with imbalanced data is {cross_val_scores.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxAo4Z9JP-R8"
      },
      "source": [
        "#**Model selection with balanced dataset**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKOaXGDq9Az1"
      },
      "source": [
        "Trying to balance a imbalanced dataset in this kind of problem and seeing whether it works for this kind of problem or not. Balancing a dataset using a Upsmapling method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Y3ZSr5RaST"
      },
      "source": [
        "# Performance of the Model is calculated using the Acurracy\n",
        "1. Multinomial Naive Bayes ---->   0.983\n",
        "2. Random Forest Classifier ---->   1.000\n",
        "3. Support Vector Machine Classifier ---->  0.997"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEQgw-fwwKl-"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "spam_messages_df = pd.DataFrame(spam_messages)\n",
        "for _ in range(5) : \n",
        "  samples = spam_messages.sample(n = 747)\n",
        "  spam_messages_df = pd.concat([spam_messages_df, samples], axis = 0)\n",
        "new_df = pd.concat([spam_messages_df, ham_messages])\n",
        "\n",
        "sns.countplot(x = new_df['Label'])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "new_df['NewMessage'] = new_df['Messages'].apply(text_preprocessing)\n",
        "new_df['NewMessageLen'] = new_df['NewMessage'].apply(len)\n",
        "new_df.drop('Messages', axis = 1, inplace = True)\n",
        "new_df['Label'] = new_df['Label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "X1 = new_df['NewMessage']\n",
        "y1 = new_df['Label']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "countvectorizer = CountVectorizer(max_features = 5000)\n",
        "X1 = countvectorizer.fit_transform(new_df['NewMessage']) .toarray()\n",
        "x1_train, x1_test,y1_train, y1_test = train_test_split(X1, y1, random_state = 0)\n",
        "\n",
        "# MultiNomial Naive Bayes\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "multinb1 = MultinomialNB()\n",
        "multinb1.fit(x1_train, y1_train)\n",
        "y1_pred = multinb1.predict(x1_test)\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"-------classification report of the Model------\")\n",
        "print(classification_report(y1_test, y1_pred))\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"-----Confusion Matrix for MultiNomial Naive Bayes with balanced data------\")\n",
        "cm = confusion_matrix(y1_test, y1_pred)\n",
        "plt.figure(figsize=(8,5))\n",
        "axis_labels = ['ham', 'spam']\n",
        "ga = sns.heatmap(data=cm, annot=True, cmap=\"Blues\", xticklabels=axis_labels, yticklabels=axis_labels, fmt='g', cbar_kws={\"shrink\": 0.5})\n",
        "plot = plt.xlabel('Actual values')\n",
        "plot = plt.ylabel('Predicted values')\n",
        "plot = plt.title('MultiNomial Naive Bayes')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "print(\"-----Result of MultiNomial Naive Bayes------\")\n",
        "scores = cross_val_score(multinb1, X1, y1, cv = 10)\n",
        "print(f\"{scores} scores of the cross validation\")\n",
        "print()\n",
        "print(f\"{np.round(scores.mean(), 3)} is the average score from cross validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuiC5o1v-OBb"
      },
      "source": [
        "After balancing the dataset, the result is quite good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm_4PKCjz6T1"
      },
      "outputs": [],
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "randomforest = RandomForestClassifier()\n",
        "randomforest.fit(x1_train, y1_train)\n",
        "y1_pred_randomforest = randomforest.predict(x1_test)\n",
        "\n",
        "# confusion matrix\n",
        "print(\"-----Confusion Matrix for Random Forest with balanced dataset----\")\n",
        "cm = confusion_matrix(y1_test, y1_pred_randomforest)\n",
        "plt.figure(figsize=(8,5))\n",
        "axis_labels = ['ham', 'spam']\n",
        "ga = sns.heatmap(data=cm, annot=True, cmap=\"Blues\", xticklabels=axis_labels, yticklabels=axis_labels, fmt='g', cbar_kws={\"shrink\": 0.5})\n",
        "plot = plt.xlabel('Actual values')\n",
        "plot = plt.ylabel('Predicted values')\n",
        "plot = plt.title('Random Forest')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(\"-----classification report-------\")\n",
        "print(classification_report(y1_test, y1_pred_randomforest))\n",
        "print()\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_scores = cross_val_score(randomforest, X1, y1, cv = 10, scoring = 'accuracy')\n",
        "print(f\"Scores for Random Forest with balanced data is {cross_val_scores}\")\n",
        "print()\n",
        "\n",
        "print(f\"Average score for the Random Forest with balanced data is {cross_val_scores.mean()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmZfAREG0DG8"
      },
      "outputs": [],
      "source": [
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "supportv_classifier = SVC()\n",
        "supportv_classifier.fit(x1_train, y1_train)\n",
        "y1_pred_svm = supportv_classifier.predict(x1_test)\n",
        "\n",
        "\n",
        "# confusion matrix\n",
        "print(\"-----Confusion Matrix for Support Vector Machine with balanced dataset----\")\n",
        "cm = confusion_matrix(y1_test, y1_pred_svm)\n",
        "plt.figure(figsize=(8,5))\n",
        "axis_labels = ['ham', 'spam']\n",
        "ga = sns.heatmap(data=cm, annot=True, cmap=\"Blues\", xticklabels=axis_labels, yticklabels=axis_labels, fmt='g', cbar_kws={\"shrink\": 0.5})\n",
        "plot = plt.xlabel('Actual values')\n",
        "plot = plt.ylabel('Predicted values')\n",
        "plot = plt.title('Support Vector Machine')\n",
        "plt.show()\n",
        "print()\n",
        "\n",
        "print(\"-----classification report-------\")\n",
        "print(classification_report(y1_test, y1_pred_svm))\n",
        "print()\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_scores = cross_val_score(supportv_classifier, X1, y1, cv = 2, scoring = 'accuracy')\n",
        "print(f\"Scores for Svm Model with balanced data is {cross_val_scores}\")\n",
        "print()\n",
        "\n",
        "print(f\"Average score for the Svm with balanced data is {cross_val_scores.mean()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Random Forest Model"
      ],
      "metadata": {
        "id": "WJrwItFcujeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "def _prediction(message) :\n",
        "  message = text_preprocessing(message)\n",
        "  test_data = countvectorizer.transform([message]).toarray()\n",
        "  prediction = randomforest.predict(test_data)\n",
        "  return \"Message is Spam!!!\" if prediction ==1 else \"Message is not Spam!!!!!\""
      ],
      "metadata": {
        "id": "BY1SgexCrtKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Predictions"
      ],
      "metadata": {
        "id": "YeX90EOVupF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_prediction(\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\")"
      ],
      "metadata": {
        "id": "cToop-qMrve2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_prediction(\"U dun say so early hor... U c already then say...\")"
      ],
      "metadata": {
        "id": "FjCJtQQTsvG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RVkh5vIGs3tV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled10 (2).ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}